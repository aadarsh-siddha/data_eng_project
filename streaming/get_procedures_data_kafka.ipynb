{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b766834-b106-4736-bede-7dfe6f509fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install confluent_kafka\n",
    "# %pip install httpx\n",
    "# %pip install attrs\n",
    "# %pip install authlib\n",
    "# %restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3635217e-8fe6-4fd3-976d-cc79b6d1af10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "import ssl\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff21abea-b764-4e69-89f5-8e335a9c72fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaTablePath = \"dbfs:/Volumes/fhir_workspace/fhir_data_bronze/\"\n",
    "checkpointPath = \"/Volumes/fhir_workspace/fhir_data_bronze/checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5c8e54-a48f-4c9a-a12b-865b83e5d4bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "confluentClusterName = \"fhir_cluster\"\n",
    "confluentBootstrapServers = dbutils.secrets.get(\"confluent\", \"bootstrap_server\")\n",
    "confluentApiKey = dbutils.secrets.get(\"confluent\", \"kafka_api_key\")\n",
    "confluentSecret = dbutils.secrets.get(\"confluent\", \"kafka_api_secret\")\n",
    "schemaRegistryUrl = dbutils.secrets.get(\"confluent\", \"schema_registry_endpoint\")\n",
    "confluentRegistryApiKey = dbutils.secrets.get(\"confluent\", \"schema_registry_api_key\")\n",
    "confluentRegistrySecret = dbutils.secrets.get(\"confluent\", \"schema_registry_api_secret\")\n",
    "confluentTopicName = \"procedures\"\n",
    "deltaTablePath = deltaTablePath + confluentTopicName\n",
    "checkpointPath = checkpointPath + confluentTopicName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c43d7195-1c48-4cf2-9205-2d09faa55d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_registry_conf = {\n",
    "    'url': schemaRegistryUrl,\n",
    "    'basic.auth.user.info': '{}:{}'.format(confluentRegistryApiKey, confluentRegistrySecret)}\n",
    "\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7464b009-0596-4000-9c49-8ddd61df79d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "binary_to_string = fn.udf(lambda x: str(int.from_bytes(x, byteorder='big')), StringType())\n",
    "streamTestDf = (\n",
    "  spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\n",
    "  .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "  .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret))\n",
    "  .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "  .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "  .option(\"subscribe\", confluentTopicName)\n",
    "  .option(\"startingOffsets\", \"latest\")\n",
    "  .option(\"failOnDataLoss\", \"false\")\n",
    "  .load()\n",
    "  .withColumn('key', fn.col(\"key\").cast(StringType()))\n",
    "  .withColumn('fixedValue', fn.expr(\"substring(value, 6, length(value)-5)\"))\n",
    "  .withColumn('valueSchemaId', binary_to_string(fn.expr(\"substring(value, 2, 4)\")))\n",
    "  .select('topic', 'partition', 'offset', 'timestamp', 'timestampType', 'key', 'valueSchemaId','fixedValue')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e017e23-b840-4b69-8d2d-2b91114afcd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_and_save(df, epoch_id):\n",
    "    fromAvroOptions = {\"mode\": \"FAILFAST\"}\n",
    "\n",
    "    def getSchema(id):\n",
    "        return str(schema_registry_client.get_schema(id).schema_str)\n",
    "\n",
    "    distinct_schema_ids = df.select(col(\"valueSchemaId\").cast(\"integer\")).distinct()\n",
    "\n",
    "    for row in distinct_schema_ids.collect():\n",
    "        current_schema_id = row.valueSchemaId\n",
    "        schema_str = getSchema(current_schema_id)\n",
    "\n",
    "        # Filter for the schema ID\n",
    "        filter_df = df.filter(col(\"valueSchemaId\") == current_schema_id)\n",
    "\n",
    "        # Parse Avro and flatten fields\n",
    "        parsed_df = filter_df.select(\n",
    "            col(\"topic\"), col(\"partition\"), col(\"offset\"), col(\"timestamp\"), col(\"timestampType\"), col(\"key\"),\n",
    "            from_avro(\"fixedValue\", schema_str, fromAvroOptions).alias(\"parsedValue\")\n",
    "        )\n",
    "\n",
    "        flat_df = parsed_df.select(\n",
    "            col(\"topic\"), col(\"partition\"), col(\"offset\"), col(\"timestamp\"), col(\"timestampType\"), col(\"key\"),\n",
    "            col('parsedValue.START').alias('start_timestamp'),\n",
    "            col('parsedValue.STOP').alias('stop_timestamp'),\n",
    "            col('parsedValue.PATIENT').alias('patient_id'),\n",
    "            col('parsedValue.ENCOUNTER').alias('encounter_id'),\n",
    "            col('parsedValue.SYSTEM').alias('system'),\n",
    "            col('parsedValue.CODE').alias('code'),\n",
    "            col('parsedValue.DESCRIPTION').alias('description'),\n",
    "            col('parsedValue.BASE_COST').alias('base_cost'),\n",
    "            col('parsedValue.REASONCODE').alias('reason_code'),\n",
    "            col('parsedValue.REASONDESCRIPTION').alias('reason_description')\n",
    "        )\n",
    "\n",
    "        # Write to Unity Catalog table (replace this with your actual catalog.schema.table)\n",
    "        flat_df.write.format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(\"fhir_workspace.fhir_data_bronze.procedures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2faa8ecd-ed4f-44eb-86ff-089bdf867d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streamTestDf.writeStream \\\n",
    "  .option(\"checkpointLocation\", checkpointPath) \\\n",
    "  .foreachBatch(parse_and_save) \\\n",
    "  .queryName(\"procedures_data\") \\\n",
    "  .start()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "get_procedures_data_kafka",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
